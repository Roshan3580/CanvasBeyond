# Image Outpainting Project
## Objective
This computer vision project tackles the intriguing challenge of extending images beyond their original edges, crafting a realistic and visually cohesive expansion.
## Methodology
<p>
While many inpainting techniques rely on Generative Adversarial Networks (GANs) and Convolutional Neural Networks (CNNs), this project zeroes in on outpainting, which goes beyond the image's borders. It utilizes a completion network along with two auxiliary context discriminators, which are only used during the training phase, not when testing.
</p>
<p>
Network Components: The architecture consists of:
- Completion Network: This is the heart of the project, responsible for extending images past their original limits.
- Global Discriminator Network: This network evaluates entire images to determine if they look real or if they’ve been completed.
- Local Discriminator Network: This one zooms in on smaller areas around the completed sections to judge their realism.
</p>

## Training
Both discriminator networks are trained to tell apart real images from those generated by the network, while the completion network is trained to fool both discriminators. This thorough training process equips the completion network to create visually believable extensions. **Architecture Efficacy:** The project leverages advanced convolutional neural networks for image extension, using a single completion network. To ensure the image completion is spot on, two additional networks—global and local discriminator networks—work together. This clever combination allows the completion network to produce realistic extensions.


## Dataset

**Source**: The dataset used in this project is the well-known Places365 dataset, carefully curated by MIT. Initially designed for CNNs to help with scene feature recognition, it fits perfectly with what we need for this project.

**Data Selection**: From the Places365 dataset, we specifically pulled a large number of beach images. These images became the core dataset for our model, with a total of 3,500 beach images utilized for training over 25 epochs.

## Experimental Results

**DCGAN Implementation:** We successfully implemented a Deep Convolutional Generative Adversarial Network (DCGAN) architecture, enabling the generation of images up to a resolution of 256 x 256 pixels.

**Image Resizing:** To handle various input image sizes, we extensively utilized the OpenCV library in Python for image resizing, ensuring compatibility with our model.

**Framework:** The project's architecture was predominantly developed using Keras, providing flexibility for incorporating multiple convolution layers within the network and working with larger image sizes.

**Outcome:** The outcomes of our experiments yielded impressive results, demonstrating the effectiveness of our DCGAN-based image generation approach.

## Results
<img width="496" alt="image" src="https://github.com/riya1606/Image_Outpainting/assets/62128029/19b16231-1197-4367-8d3b-0e1f270d93b8">

## Conclusion
In conclusion, we have successfully developed a comprehensive end-to-end Generative Adversarial Network (GAN) capable of addressing image outpainting challenges. Our deep neural network implementation extends images up to 256 × 256 pixels, producing high-quality results. By introducing a local discriminator in addition to the global discriminator, we achieved near-ideal image outputs. Future work could involve expanding the project's capabilities to accept larger input image sizes and enhancing image resolution, contingent on increased computational resources.
